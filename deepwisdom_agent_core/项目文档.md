# DeepWisdom Agent 项目文档

## 1. 项目概述

DeepWisdom Agent 是一个基于 LangChain 和 LangGraph 构建的智能对话代理，具有以下核心功能：

- 基础对话功能：支持与用户进行自然语言交互
- 记忆系统：能够提取、存储和检索用户相关信息
- 本地搜索：支持搜索项目文档目录中的内容
- 工具集成：通过 MCP 协议可扩展集成额外工具

## 2. 项目结构

项目采用模块化设计，主要文件结构如下：

```
deepwisdom_agent/
├── main.py                 # 主程序入口
├── graph.py                # Agent 工作流图构建
├── memory.py               # 记忆存储系统
├── search_tool.py          # 本地文档搜索工具
├── mcp_client.py           # MCP 客户端实现
├── requirements.txt        # 项目依赖
├── config.example.ps1      # 配置示例文件
└── ToT-24-Game-Solver/     # 独立的 24 点游戏求解器（本项目不包含此部分）
```

## 3. 核心功能实现

### 3.1 基础对话功能

基础对话功能通过 `main.py` 和 `graph.py` 实现：

- **main.py**：处理用户输入和输出，管理对话循环，定期提取记忆
- **graph.py**：构建 LangGraph 工作流，实现 Agent 决策逻辑

主要流程：
1. 用户输入被添加到对话历史
2. Agent 根据对话历史和记忆生成响应
3. 如果需要，Agent 可以调用工具（如本地搜索）
4. 响应返回给用户
5. 定期从对话中提取记忆并存储

```python
# graph.py 中的 Agent 状态定义
class AgentState(TypedDict):
    messages: Annotated[list, add_messages]

# main.py 中的对话循环
while True:
    user_input = input("You> ").strip()
    # 处理用户输入...
    state = {"messages": messages + [HumanMessage(content=user_input)]}
    result = app.invoke(state)
    # 处理响应...
```

### 3.2 记忆系统

记忆系统通过 `memory.py` 实现，支持四种记忆类型：

- **preference**：用户偏好（如语言、风格）
- **profile**：用户资料（如角色、项目）
- **constraint**：约束条件（如截止日期、预算）
- **fact**：事实信息（如使用的技术、操作系统）

记忆存储在 SQLite 数据库（`memory.sqlite`）中，具有以下功能：

1. **记忆提取**：使用 LLM 从对话中提取结构化记忆
2. **记忆存储**：支持单个和批量记忆的插入/更新
3. **记忆检索**：基于关键词的记忆检索
4. **记忆格式化**：将记忆格式化为适合注入系统提示的文本

```python
# memory.py 中的记忆提取函数
def extract_memories_with_llm(llm, conversation_history: str) -> List[MemoryEntry]:
    # 使用 LLM 从对话中提取记忆
    # ...

# memory.py 中的记忆存储类
class MemoryStore:
    def upsert_many(self, entries: List[MemoryEntry]):
        # 批量插入/更新记忆
        # ...
    
    def retrieve(self, query: str, top_k: int = 5) -> List[MemoryEntry]:
        # 检索相关记忆
        # ...
```

### 3.3 搜索功能

搜索功能通过 `search_tool.py` 实现，提供本地文档搜索能力：

- 搜索范围：项目的 `docs/` 目录
- 支持的文件类型：`.txt`、`.md`、`.markdown`
- 搜索方式：基于关键词匹配的全文搜索
- 结果格式：返回包含来源和摘要的 JSON 格式结果

```python
# search_tool.py 中的搜索工具
@tool
def local_search(query: str, top_k: int = 3) -> str:
    """Search docs/ for keyword matches and return top_k snippets with sources."""
    # 搜索文档并返回结果
    # ...
```

### 3.4 工具使用

工具使用通过 `graph.py` 和 `mcp_client.py` 实现：

- **graph.py**：将工具绑定到 LLM，构建包含工具调用的工作流
- **mcp_client.py**：实现 MCP 协议客户端，支持从远程服务器加载额外工具

主要特点：

1. **工具集成**：支持将本地工具和 MCP 工具无缝集成
2. **动态工具调用**：Agent 可以根据需要自动调用工具
3. **工具结果处理**：工具调用结果会被自动整合到对话中

```python
# graph.py 中的工具集成
all_tools = [local_search]
if extra_tools:
    all_tools.extend(extra_tools)

llm_with_tools = llm.bind_tools(all_tools)

# mcp_client.py 中的工具包装
def wrap_as_langchain_tools(self) -> List[StructuredTool]:
    # 将 MCP 工具包装为 LangChain 工具
    # ...
```

## 4. 使用方法

### 4.1 环境配置

1. 安装依赖：

```bash
pip install -r requirements.txt
```

2. 配置环境变量（复制 `config.example.ps1` 为 `config.ps1` 并修改）：

```powershell
# OpenAI API 密钥
$env:OPENAI_API_KEY = "your-api-key"

# 可选：MCP 服务器配置
# $env:MCP_SERVER_COMMAND = "npx"
# $env:MCP_SERVER_ARGS = '["@modelcontextprotocol/server-everything"]'
```

3. 激活环境变量：

```bash
.\config.ps1
```

### 4.2 运行程序

执行主程序：

```bash
python main.py
```

程序启动后，会显示提示信息：

```
[Memory] Initialized SQLite memory store
[MCP] No MCP server configured (set MCP_SERVER_COMMAND to enable)
Type 'exit' or 'quit' to stop.
```

### 4.3 基本操作

1. **对话**：输入自然语言与 Agent 交互

```
You> 你好
Agent> 你好！我是你的助手，有什么可以帮助你的？
```

2. **搜索**：当需要查询本地文档时，Agent 会自动调用搜索工具

```
You> 请告诉我关于记忆系统的信息
Agent> 根据本地文档，记忆系统使用 SQLite 数据库存储用户相关信息... [source: memory.md]
```

3. **退出**：输入 `exit` 或 `quit` 退出程序

### 4.4 高级功能

#### 4.4.1 配置 MCP 工具

如果需要使用 MCP 工具，可以在 `config.ps1` 中配置 MCP 服务器：

```powershell
$env:MCP_SERVER_COMMAND = "npx"
$env:MCP_SERVER_ARGS = '["@modelcontextprotocol/server-everything"]'
```

#### 4.4.2 文档管理

将需要搜索的文档放入项目的 `docs/` 目录，支持 `.txt`、`.md`、`.markdown` 格式。

## 5. 技术细节

### 5.1 对话流程

对话流程通过 LangGraph 实现，包含两个主要节点：

1. **agent** 节点：调用 LLM 生成响应
2. **tools** 节点：执行工具调用

```python
# graph.py 中的工作流构建
graph = StateGraph(AgentState)
graph.add_node("agent", call_model)
graph.add_node("tools", ToolNode(all_tools))
graph.set_entry_point("agent")
graph.add_conditional_edges("agent", should_continue, {"tools": "tools", END: END})
graph.add_edge("tools", "agent")
```

### 5.2 记忆提取机制

记忆提取采用定期机制，每 3 轮对话（6 条消息）提取一次记忆：

```python
# main.py 中的记忆提取逻辑
if len(conversation_buffer) >= 6:
    _extract_and_store_memories(llm, memory_store, conversation_buffer)
    conversation_buffer.clear()
```

### 5.3 工具调用决策

Agent 根据 LLM 的输出决定是否调用工具：

```python
# graph.py 中的工具调用决策
def should_continue(state: AgentState):
    last = state["messages"][-1]
    if getattr(last, "tool_calls", None):
        return "tools"
    return END
```

## 6. 扩展与定制

### 6.1 添加自定义工具

可以在 `graph.py` 中添加自定义工具：

```python
# 导入自定义工具
from my_tool import my_custom_tool

# 在构建图时添加
all_tools = [local_search, my_custom_tool]
```

### 6.2 修改记忆提取逻辑

可以在 `memory.py` 中修改记忆提取的 prompt 或逻辑：

```python
# 修改记忆提取 prompt
extraction_prompt = f"""新的记忆提取提示...
{conversation_history}
"""
```

## 7. 故障排除

### 7.1 常见问题

1. **OpenAI API 错误**：
   - 检查 API 密钥是否正确配置
   - 确认网络连接正常

2. **记忆提取失败**：
   - 查看控制台输出的错误信息
   - 检查 LLM 模型是否支持记忆提取功能

3. **搜索无结果**：
   - 确认 `docs/` 目录存在且包含文档
   - 检查搜索关键词是否正确

### 7.2 日志信息

程序运行时会输出以下日志信息，有助于调试：

```
[Memory] Initialized SQLite memory store
[MCP] Loaded 2 tools from MCP server
[Memory] Extracted and stored 3 memories
[Memory] Extraction failed: [错误信息]
```

## 8. 更新日志

- v0.1.0：初始版本，实现基础对话、记忆系统和本地搜索
- v0.1.1：添加 MCP 工具集成支持
- v0.1.2：优化记忆提取逻辑和对话流程

## 9. 联系方式

如有问题或建议，请联系项目维护人员。